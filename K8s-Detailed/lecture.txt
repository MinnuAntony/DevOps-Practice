Rolling Update:
A rolling update allows a Deployment update to take place with zero downtime.
It does this by incrementally replacing the current Pods with new ones.
The new Pods are scheduled on Nodes with available resources, and Kubernetes waits for those new Pods to start before removing the old Pods.

To roll back the deployment to your last working version, use the rollout undo subcommand:
kubectl rollout undo deployments/kubernetes-bootcamp
The rollout undo command reverts the deployment to the previous known state (v2 of the image).
Updates are versioned and you can revert to any previously known state of a Deployment.

Vertical Pod Autoscaler (VPA) :  can either increase or decrease the CPU and memory allocated to each pod,
Horizontal Pod Autoscaler (HPA):  can replicate or terminate pods, thus affecting the total pod count. 
Affecting the cluster capacity as a whole, the Cluster Autoscaler (CA) adds or removes nodes dedicated to
the cluster to provide the appropriate amount of computing resources needed to host the desired workloads

To check metrics, HPA depends on another Kubernetes resource known as the Metrics Server. 
The Metrics Server provides standard resource usage measurement data 

Label:
        Label: A key-value pair that is attached to objects (pods, nodes, etc.) to organize and select them. Labels are used for grouping and filtering.

CNI (Container Network Interface)
		    Flannel: 
				Flannel is a popular CNI plugin used for networking in Kubernetes clusters. It provides a simple and easy-to-understand overlay network for container communication.
			Calico: 
				Calico is a CNI plugin that supports various networking modes, including Layer 3 routing and BGP. It is designed for scalability and is often used in large-scale Kubernetes deployments.
			Weave: 
				Weave is another CNI plugin that provides a simple and lightweight overlay network for Kubernetes clusters. It allows for easy communication between containers across nodes.

ClusterIP: 
Default service type in Kubernetes.
Exposes the Service only inside the cluster.
It gets a virtual IP address (cluster-internal) that other Pods inside the cluster can use to talk to it.
Cannot be accessed from outside the cluster directly.
Use case: Internal communication between microservices (e.g., backend â†” database).

NodePort
Exposes the Service on each Nodeâ€™s IP at a static port (between 30000â€“32767).
You can access the Service from outside the cluster using:
<NodeIP>:<NodePort>
Internally, NodePort forwards traffic to the ClusterIP (so every NodePort service also has a ClusterIP behind it).


K8S NETWORKING:

1 . Two containers in the same Pod (same node)

Key idea: All containers in a Pod share the same Linux network namespace. They see the same lo (loopback), same eth0, same Pod IP and routes.
How they talk
They can talk over localhost (127.0.0.1)` or the Pod IP because theyâ€™re in the same netns.
No NAT, no kube-proxy, no iptables/ipvs involved.
Often they use localhost TCP ports or Unix domain sockets on a shared volume (sidecar pattern).
Internals
Kubelet creates the Pod sandbox netns.
CNI attaches a veth pair: eth0 inside the Pod â†” a veth peer on the host.
The Pod gets one IP; all its containers use it.
-----------------------------

Q1. Two containers in the same Pod
They share the same network namespace â†’ same IP, same eth0.
They talk via localhost (127.0.0.1).
CBR / plugin / kube-proxy not involved.

Q2. Two Pods on the same Node
Each Pod has a veth pair connected to the nodeâ€™s CBR switch.
Packets flow directly via that bridge switch.
Only CBR switch involved.
Plugin created the bridge, but not in the data path.
kube-proxy not involved.

Q3. Two Pods on different Nodes
Pod traffic leaves Node1â€™s CBR switch.
CNI plugin handles inter-node delivery:
Overlay plugins (Flannel VXLAN, Weave) â†’ encapsulate, tunnel, decapsulate.
Routing plugins (Calico BGP, Cilium) â†’ route directly.
Packet enters Node2â€™s CBR switch and is delivered to PodB.
Path = CBR switch + CNI plugin.
kube-proxy not involved (direct Pod-to-Pod).

Q4. Service across Nodes
Pod sends to ServiceIP.
kube-proxy rewrites (DNAT) ServiceIP â†’ backend PodIP.
Then delivery works just like Q2 or Q3:
If backend Pod is local â†’ via same-node CBR switch.
If backend Pod is remote â†’ via CBR switch + CNI plugin.
Path = kube-proxy + CBR switch + (plugin if needed)


----------aug 21 2025------------------

points to discuss : ingress, RBAC,NEetwork policies, secrets,config maps,cron jobs,jobs, probes , volumes
- INGRESS: 
In a non-managed cluster, we use Ingress to expose multiple services through a single external IP and manage routing (like example.com/api â†’ backend, example.com/app â†’ frontend) instead of opening separate NodePorts/LoadBalancers for each service.


Statefulset :

Why we need it: To run applications that require stable identity, storage, and ordered startup â†’ things like databases, message brokers, distributed systems.
What it is: A Kubernetes controller that provides each pod a unique, persistent identity and storage, ensuring predictable behavior for stateful apps.

# ðŸ“˜ StatefulSet â€“ Summary

### ðŸ”¹ What is a StatefulSet?

A **StatefulSet** is a Kubernetes controller used to manage **stateful applications** â€” applications that require **stable network identity, persistent storage, and ordered startup/shutdown**.

* Pods get **predictable names** (`app-0`, `app-1`â€¦),
* Each pod has its own **PersistentVolumeClaim (PVC)**,
* Pods start and stop **in sequence**,
* Often paired with a **Headless Service** (`clusterIP: None`) for stable DNS.

---

### ðŸ”¹ When do we use StatefulSets?

We use StatefulSets when running workloads that **cannot be treated as identical and replaceable**, for example:

* Databases (MySQL, PostgreSQL, MongoDB)
* Messaging systems (Kafka, RabbitMQ)
* Coordination services (Zookeeper, etcd)
* Any **clustered/distributed app** where pods have distinct roles.

---

### ðŸ”¹ Why do we use StatefulSets?

Because stateful apps need:

1. **Stable Identity** â€“ pods must keep the same name/hostname so cluster members (primary, replicas) can recognize each other.
2. **Stable Storage** â€“ each podâ€™s data must persist across restarts and rescheduling.
3. **Ordered Deployment/Scaling** â€“ some systems need a leader to start first, then followers.
4. **Direct Pod Access** â€“ headless service enables connecting to specific pods (like `db-0` as primary).

---

âœ… **In short:**

* **What:** StatefulSet = controller for managing stateful apps in Kubernetes.
* **When:** Use it for databases, queues, and distributed systems that need persistent identity & storage.
* **Why:** To ensure data consistency, cluster stability, and predictable pod behavior across restarts.

---

---> The 4 Default Namespaces in Kubernetes

1. default
Where your resources go if you donâ€™t specify a namespace.
Example: if you just run kubectl run nginx --image=nginx, it gets created in default.

2. kube-system
Reserved for Kubernetes system components.
Runs core pods like:
kube-dns / coredns (cluster DNS)
kube-proxy
Control plane add-ons
You usually donâ€™t deploy your apps here.

3. kube-public
Publicly readable namespace.
Contains resources that need to be accessible by all users (even unauthenticated ones).
Example: cluster-wide info like ConfigMaps for cluster configuration.
Rarely used for apps.

4. kube-node-lease
Stores lease objects for each node.
Used for node heartbeats â†’ improves scalability of node health checks.

Introduced in newer Kubernetes versions (v1.13+).



---------------------------22/08/25
































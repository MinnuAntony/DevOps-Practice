Why we use Ingress
------------------------
Without Ingress:
Each service needs its own NodePort or LoadBalancer to be externally accessible.
This exposes multiple IPs/ports, which is messy and hard to manage.
With Ingress:
Only the Ingress Controller’s Service is exposed externally.
All services inside the cluster are accessed through the controller, using paths or hostnames defined in Ingress rules.
Clients don’t need to know the internal service IPs or NodePorts.

-------------------------------------------------------------------------------------------
Service Account:
by deafult there is a sa callled default
  from a pod you cannot make API CALLS - actually there is no need for that


 kubectl create rolebinding nginx-sa-readonly \
  --clusterrole=view \
  --serviceaccount=default:nginx-serviceaccount \
  --namespace=default
here cluster role is set to view.
Role and rolebinding



--------------------------------------------------------------------------------------
Liveness Probe
A liveness probe checks if your container is still running and healthy. If the probe fails, Kubernetes will automatically restart the container. This is particularly useful for detecting deadlocks, where an application is running but unable to make progress.

Readiness Probe
A readiness probe checks if your container is ready to start accepting traffic. If the probe fails, Kubernetes will stop sending traffic to the pod. Once the probe succeeds again, the pod will be marked as ready and begin receiving traffic. This is useful for applications that require time to start up, such as loading large data sets or connecting to external databases.

Probes work by performing periodic checks on a container to determine its health and readiness. Kubernetes's agent, the kubelet, runs these checks and takes a predefined action based on the outcome. There are three primary types of probes: liveness, readiness, and startup. All three types can be configured to perform one of three kinds of checks:

HTTP GET: Sends an HTTP request to a specific path and port on the container. The probe succeeds if the response status code is between 200 and 399.

TCP Socket: Attempts to open a TCP connection to a specified port. The probe succeeds if the connection is established.

Exec: Executes a command inside the container. The probe succeeds if the command exits with a status code of 0.

How Probes are Configured and Used
Each probe type is configured with a set of parameters that dictate its behavior. Here's a breakdown of the key parameters:

initialDelaySeconds: The time (in seconds) the kubelet waits before starting the first probe. This is crucial for giving the container enough time to start up before checks begin.

periodSeconds: The interval (in seconds) between each probe check.

timeoutSeconds: The maximum time (in seconds) the probe is allowed to wait for a response before it's considered a failure.

failureThreshold: The number of consecutive failed attempts required for the probe to be considered a definitive failure.

Liveness Probe
The liveness probe checks if a container is still "alive" and functioning correctly. If it fails, Kubernetes restarts the container. This is useful for catching deadlocks, where an application is running but can't make progress. The failureThreshold parameter determines how many consecutive failures will trigger a restart.

Readiness Probe
The readiness probe checks if a container is ready to serve traffic. If it fails, Kubernetes stops sending traffic to the pod by removing its IP address from the service endpoints. This is useful for applications that need to load data, establish connections, or perform other initialization tasks before they can handle requests. Once the readiness probe starts succeeding again for a configured number of times (successThreshold), the pod's IP is added back to the service.

Startup Probe
A startup probe is designed for applications with a long startup time. If a startup probe is configured, it will run first and disable the liveness and readiness probes until it succeeds. This prevents the other probes from prematurely restarting or removing a slow-starting container from service. Once the startup probe succeeds, the regular liveness and readiness checks begin.


frontend.yaml:
Based on the YAML provided, the liveness and readiness probes check the health of the frontend container by sending HTTP GET requests to port 80. These checks ensure the application is running and ready to handle traffic.

Liveness Probe
The liveness probe checks if the container is "alive" and functioning.

httpGet: The probe performs an HTTP GET request.

path: /: The request is sent to the root path of the application.

port: 80: The request is sent to port 80, which is where the container is configured to listen.

initialDelaySeconds: 10: The first check will occur 10 seconds after the container starts. This gives the application time to initialize.

periodSeconds: 15: After the first check, Kubernetes will check the container's health every 15 seconds.

failureThreshold: 3: If the probe fails three consecutive times, Kubernetes will consider the container unhealthy and restart it.

Readiness Probe
The readiness probe checks if the container is ready to accept and process traffic.

httpGet: The probe performs an HTTP GET request.

path: /: The request is sent to the root path of the application.

port: 80: The request is sent to port 80.

initialDelaySeconds: 5: The first check will occur 5 seconds after the container starts.

periodSeconds: 10: After the first check, Kubernetes will check the container's readiness every 10 seconds.

failureThreshold: 3: If the probe fails three consecutive times, Kubernetes will stop sending traffic to the pod. The pod will be marked as "unready" in the service endpoints, and no new connections will be routed to it until the probe succeeds again.


If you install kube-prometheus-stack Helm chart:

It sets up Prometheus with Kubernetes service discovery.

Deploys node-exporter as a DaemonSet on every node.

Prometheus automatically finds those node-exporter pods and scrapes them.

So you don’t need to “hardcode” nodes. Prometheus uses Kubernetes API + labels to discover and monitor everything running in your cluster.


---------------------------------------------------------------------------------


Perfect! Let’s make a **clear comparison table** for your three workflows (Python / Node / Go) showing **tools used, purpose, and why chosen per language**.

| **Pipeline Step / Tool**                | **User Service (Python)**                | **Frontend (Node.js / React)**           | **Expense Service (Go)**                   | **Purpose / Reason**                                                                   |
| --------------------------------------- | ---------------------------------------- | ---------------------------------------- | ------------------------------------------ | -------------------------------------------------------------------------------------- |
| **Checkout Code**                       | `actions/checkout@v3`                    | `actions/checkout@v3`                    | `actions/checkout@v3`                      | Standard GitHub Action to fetch repo                                                   |
| **Setup Runtime**                       | `setup-python@v4`, Python 3.11           | `setup-node@v3`, Node 20                 | `setup-go@v4`, Go 1.21                     | Provides correct language environment for builds/tests                                 |
| **Dependency Install**                  | `pip install -r requirements.txt`        | `npm ci`                                 | `go mod tidy`                              | Ensures project dependencies are installed and consistent                              |
| **Linting / Code Style**                | `flake8`                                 | `eslint`                                 | `golint`, `staticcheck`                    | Ensures code style, detects simple bugs and bad practices                              |
| **Unit Testing**                        | `pytest`                                 | `jest` (`npm test`)                      | Go built-in tests (`go test` can be added) | Validates code correctness                                                             |
| **SAST (Static Analysis)**              | `bandit`                                 | `eslint security rules`                  | `gosec`                                    | Detects security issues in code (hardcoded secrets, unsafe function calls, injections) |
| **SCA (Dependency Vulnerability Scan)** | `pip-audit`                              | `npm audit`                              | Can use `trivy fs` (not implemented)       | Detects known CVEs in project dependencies                                             |
| **License / Compliance Check**          | N/A                                      | `license-checker`                        | N/A                                        | Ensures third-party libraries’ licenses are acceptable                                 |
| **Docker Build**                        | `docker build` (multi-tag: SHA + latest) | `docker build` (multi-tag: SHA + latest) | `docker build` (multi-tag: SHA + latest)   | Package service into container for deployment                                          |
| **Container Vulnerability Scan**        | `trivy image`                            | `trivy image`                            | `trivy image`                              | Detects CVEs in base image + installed packages before pushing to registry             |
| **Push Docker Image**                   | DockerHub                                | DockerHub                                | DockerHub                                  | Publishes image for deployment                                                         |
| **Update Deployment Manifest**          | `sed + git commit`                       | `sed + git commit`                       | `sed + git commit`                         | Updates image SHA in k8s manifest for GitOps (ArgoCD)                                  |
| **Secrets Scan (Optional)**             | N/A                                      | N/A                                      | N/A                                        | Could use `trivy fs --scanners secret .` to detect accidental secrets                  |

---

### ✅ Key Takeaways

1. **Language-specific tools**:

   * Python → `flake8`, `bandit`, `pip-audit`
   * Node → `eslint`, `eslint security plugin`, `npm audit`
   * Go → `golint`, `staticcheck`, `gosec`

2. **Common DevSecOps steps**:

   * Docker build → Trivy scan → push → update k8s manifest

3. **Why chosen**:

   * Each tool matches the **ecosystem/language best practices**.
   * Combining SAST + SCA + image scan gives layered security.
   * GitOps update ensures automatic deployment without manual edits.

---


Absolutely! Let’s break this Terraform configuration down **step by step**. It’s basically setting up a **remote backend for Terraform state management** in AWS using **S3 and DynamoDB**.

---

### 1️⃣ AWS Provider

```hcl
provider "aws" {
  region = "us-east-1"
}
```

* This tells Terraform which cloud provider to use — in this case, AWS.
* `region = "us-east-1"` specifies the AWS region where resources will be created.

---

### 2️⃣ S3 Bucket

```hcl
resource "aws_s3_bucket" "tf_state" {
  bucket = "minnu-terraform-state-bucket"

  tags = {
    Name = "Minnu-Terraform-State"
  }
}
```

* Creates an **S3 bucket** to store Terraform state files (`.tfstate`).
* `bucket` is the unique name of the S3 bucket.
* `tags` are just metadata, useful for organizing and identifying resources.

**Why S3?**

* S3 allows you to **store Terraform state remotely**, making it accessible for your team and safer than local storage.

---

### 3️⃣ Bucket Versioning

```hcl
resource "aws_s3_bucket_versioning" "tf_state_versioning" {
  bucket = aws_s3_bucket.tf_state.id

  versioning_configuration {
    status = "Enabled"
  }
}
```

* Enables **versioning** on the S3 bucket.
* This keeps **previous versions of your Terraform state files**.
* Useful if something goes wrong — you can **rollback** to an older state.

---

### 4️⃣ Bucket Server-Side Encryption (SSE)

```hcl
resource "aws_s3_bucket_server_side_encryption_configuration" "tf_state_sse" {
  bucket = aws_s3_bucket.tf_state.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}
```

* Configures **server-side encryption** on the S3 bucket.
* `AES256` means AWS will encrypt the objects using **AES-256** encryption automatically.
* Keeps your Terraform state **secure at rest**, because it might contain sensitive info (like secrets or passwords).

---

### 5️⃣ DynamoDB Table for Terraform State Locking

```hcl
resource "aws_dynamodb_table" "tf_lock" {
  name         = "minnu-terraform-state-lock"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }

  tags = {
    Name = "Minnu-Terraform-Lock"
  }
}
```

* Creates a **DynamoDB table** used for **state locking**.
* Terraform uses this to prevent **multiple people from applying changes at the same time**, which could corrupt the state.
* `hash_key = "LockID"` is the primary key of the table.
* `billing_mode = "PAY_PER_REQUEST"` means you only pay for the requests made — no need to provision throughput.

---

### ✅ Summary

* **S3 bucket**: Stores your Terraform state files remotely.
* **Versioning**: Keeps past versions of state for recovery.
* **SSE**: Encrypts state files at rest.
* **DynamoDB**: Ensures only one Terraform process updates the state at a time (locking).

This is a **best practice setup** for collaborative Terraform workflows.

---

If you want, I can also show you **how to configure Terraform to actually use this S3 bucket and DynamoDB table as a backend**, so your state is fully remote and safe.

Do you want me to do that?

------






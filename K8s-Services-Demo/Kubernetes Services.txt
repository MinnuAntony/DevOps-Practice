Kubernetes Services

First we explain shortly what a Service component is in Kubernetes and when we need it.
Then we go through different Service types:ClusterIP,NodePort,and LoadBalancer.
We explain the differences and when to use which one

-- define service

before understanding the definition lets understand why do we need services.

In a Kubernetes cluster each pod gets its own internal ip address but the pods in Kubernetes
are ephemeral meaning that they come and go very frequently and when the pod restarts or when 
old one dies and the new one gets started in its place, it gets a new ip address
So it doesn't make sense to use pod ip addresses directly because then you would have to
adjust that every time .the pod gets recreated

With the service however you have a solution of a stable or static ip address that
stays even when the pod dies. So basically in front of each pod we set a service which represents a
persistent stable ip address to access that pod. A service also provides load balancing
because when you have pod replicas, for example three replicas of your micro
service application or three replicas of mysql application, the service will basically get each
request targeted to that mysql or your microservice
application and then forward it to one of those pods. So clients can call a single
stable ip address instead of calling each pod individually.  so services are a good
abstraction for loose coupling for communication within the cluster
and  also from external services like if you have browser requests coming to the cluster
or if you're talking to an external database for example


{{{
How Pod IPs Are Assigned
Kubernetes uses the CNI (Container Network Interface) plugin to manage Pod networking.
Each Pod gets an IP from the cluster’s Pod network CIDR (like 10.244.0.0/16 for Flannel).
When the Pod dies, its IP is released back to the pool.
A new Pod → new IP allocation}}}

A Service is an API abstraction in Kubernetes that exists at the cluster level.
It uses kube-proxy running on each Node to handle traffic routing:
kube-proxy ensures that traffic sent to the Service’s IP (ClusterIP) is forwarded to the appropriate Pod IPs, which may be on any node


or 

 Services: Making Pods Accessible

- Pods have Ips but there is an issue
	- Pods ips change when recreated 
	- we have multiple pods behind a deployment
	- we need a single stable way to access the application 
- A Service is like a permanent address for your pods. 
	- It provides a static ip address that doesn't change
	- load balancing across multiple pods
	- Other apps can find the service by name 
	- It has different ways to expose an application

## ⚡ Key Terms in a Kubernetes Service

1. **port**

   * This is the **port exposed by the Service itself** inside the cluster (the “Service port”).
   * Clients (other Pods or external users depending on Service type) connect to this port.
   * Example from NodePort YAML:

     ```yaml
     port: 80
     ```

     * Other Pods in the cluster can access this Service at **port 80**.

2. **targetPort**

   * This is the **port on the Pod/container** that the Service routes traffic to.
   * Can be the same as `port` or different.
   * Example:

     ```yaml
     targetPort: 8080
     ```

     * Service receives traffic on **port 80** and forwards it to **Pod's port 8080**.

3. **nodePort**

   * Only used for **NodePort** or **LoadBalancer** Services.
   * Exposes the Service on **every Node’s IP** at this static port.
   * Example:

     ```yaml
     nodePort: 30080
     ```

     * External clients can access the service at `http://<NodeIP>:30080`.
   * Kubernetes automatically forwards this to `port`, which then forwards to `targetPort` on Pods.



* **port** → Service listens here.
* **targetPort** → Pods listen here.
* **nodePort** → External access on node’s IP (optional, NodePort/LoadBalancer only).



//////cluster
ip type
this is the default type of a service
meaning when you create
a service and not specify a type
it will automatically take cluster ip as
a type so let's see how
cluster ip works and where it's used
in kubernetes setup imagine we have
a micro service application deployed in
the cluster
so we have a pod with microservice
container
running inside that pod and beside that
microservice container we have
a sidecar container that collects the
logs of the microservice and then sends
that to
some destination database so these two
containers are running in the pod
and let's say your microservice
container is running at pod
3000 and your logging container
let's say is running on port 9000
this means that those two ports will be
now open
and accessible inside the pod and pod
will also get an ip address
from a range that is assigned to a node
so the way it works is that if you have
for example three worker nodes in your
kubernetes cluster
each worker node will get a range of ip
addresses
which are internal in the cluster so for
example the pod 1 will get ip addresses
from a range of 10.2.1
onwards the second worker node will get
this ip range
and the third worker node will get this
one so let's say this port starts on
node
2 so it get an ip address
that looks like this if you want to see
the ip addresses
of your pods in the cluster you can
actually
check them using kubectl get pod output
wide command where you will get some
extra information about the pods
including its ip address and here you
will see the ip address that it got
assigned
and as i mentioned these are from the ip
address range
that each worker node in the cluster
will get so this is from the first
worker node
and these are from the second worker
node
so now we can access those containers
inside the pod at this ip address
at these ports if we set the replica
count to two
we're gonna have another pod which is
identical to the first one
which will open the same ports and it
will get a different ip address
let's say if it starts on worker node 1
you will get an ip address that looks
something like this
now let's say this micro service is
accessible through a browser
so we have ingress configured and
the requests coming in from the browser
to the micro service
will be handled by ingress how does this
incoming request
get forwarded from ingress all the way
to the pod
and that happens through a service a
cluster ip
or so-called internal service a service
in kubernetes is a component just like a
pod but it's not a process
it's just an abstraction layer that
basically represents an ip address
so service will get an ip address that
it is accessible
at and service will also be accessible
at a certain
port let's say we define that port to be
3200
so ingress will talk to the service or
hand over the request
to the service at this ip address at
this
port so this is how service is
accessible within the cluster so the way
it works is that
we define ingress rules that forward the
request
based on the request address
to certain services and we define the
service by its
name and the dns resolution
then maps that service name to an ip
address that the service
actually got assigned so this is how
ingress
knows how to talk to the service if you
don't know how ingress works i have a
separate video
where i explain ingress all its concepts
and how to
use that so you can check out that video
to learn
more on ingress itself so once the
request gets
handed over to the service at this
address
then service will know to forward that
request to
one of those pods that are registered as
the service endpoints now here are two
questions
how does service know which pods it is
managing or
which parts to forward the request to
and the second one is how does service
know which
port to forward that request
to on that specific pod the first one is
defined by selectors a service
identifies
its member pods or its endpoint parts
using selector attribute so in the
service specification
in the yaml file from which we create
the service we specify the selector
attribute
that has a key value pairs defined as a
list
now these key value pairs are basically
labels
that pots should have to match that
selector
so in the pod configuration file we
assign the parts certain labels in the
metadata section
and these labels can be arbitrary name
so we can say
my app for example and give it some
other labels
this is basically something that we
define ourselves we can give it any name
that we want
these are just key value pairs that
identify a set of pots
and in the survey cml file then we
define a selector
to match any part that has all of these
labels
this means if we have a deployment
component that creates three replicas of
parts
with label called app my app
and type microservice for example
and in the service selector attribute we
define those two labels
then service will match all of those
three
pod replicas and it will register all
three parts as its
endpoints and as i said it should match
all the selectors not just one
so this is how service will know which
parts
belong to it meaning where to forward
that
request to the second question was if a
pod has
multiple ports open where two different
applications are listening inside the
pod
how does service know which port to
forward the request to
and this is defined in the target port
attribute
so this target port attribute so let's
say target port
in our example is three thousand what
this means is that when we create the
service
it will find all the parts that match
this selector
so these pods will become endpoints of
the service
and when the service gets a request it
will pick one of those
pod replicas randomly because it's a
load balancer
and it will send the request it received
to that specific pod on a port
defined by target port attribute
in this case three thousand also note
that
when you create a service kubernetes
creates an
endpoints object that has the same name
as
the service itself and kubernetes will
use this endpoints object to keep track
of which pods are members of the service
or as i said which pods are the end
points
of the service and since this is dynamic
because whenever you create a new pod
replica or a pod dies
the end points get updated so this
object will basically track that
and note here that the service port
itself is arbitrary so you can define it
yourself
whereas the target port is not arbitrary
it has to match
the port where container the application
container inside the pod
is listening at now let's say our micro
service application
got its requests from the browser
through ingress
and internal cluster ip service and now
it needs to communicate with the
database
to handle that request for example and
in our example let's assume that the
microservice application uses mongodb
database
so we have two replicas of mongodb in
the cluster
which also have their own service
endpoint
so mongodb service is also of cluster ip
and it has its own ip address so now the
microservice application inside the pod
can talk to the mongodb database also
using the service endpoint so the
request will come from one of the parts
that gets the request
from the service to the mongodb service
at this ip address and the port that
service has
open and the service will again select
one of those pod
replicas and forward that request
to the selected pod at
the port the target port defined here
and this is the port where mongodb
application inside the pod
is listening at

when we define a service configuration
we can
specify a type of the service and the
type attribute
can have three different values it could
be cluster ip which is a default
that's why we don't have to specify that
we have a node port and load balancer
so type node port basically creates a
service that is accessible
on a static port on each worker node
in the cluster now to compare that to
our previous example
the cluster ip service is only
accessible
within the cluster itself so no external
traffic can
directly address the cluster ip service
the node port service however makes the
external traffic accessible on static
or fixed port on each worker node
so in this case instead of ingress the
browser request will come directly
to the worker node at the port that
the service specification defines
and the port that node port service type
exposes
is defined in the node port attribute
and here note that the note port value
has a predefined range between thirty
thousand
and thirty two thousand seven hundred
sixty seven
so you can have one of the values
from that range as a node port value
anything outside that range won't be
accepted so
this means that the node port service is
accessible for the external traffic like
browser request for example at ip
address of the worker node
and the node port defined here however
just like in cluster ip we have a port
of the service so when we create the
node port service a cluster ip service
to which the node port service will
route
is automatically created and here as you
see
if i list the services the note port
will have a cluster ip address
and for each ip address it will also
have the ports
open where the service is accessible at
and also note that service spends all
the worker nodes
so if you have three pod replicas
on three different nodes basically the
service will be able to handle that
request
coming on any of the worker nodes and
then forward it to one of those
pod replicas now that type of service
exposure is not very efficient and also
not secure because you're basically
opening the ports
to directly talk to the services on each
worker
node so the external clients basically
have access to the worker nodes directly
so if we gave all the services
this node port service type then we
would have a bunch of
ports open on the worker nodes clients
from outside can directly talk to
so it's not very efficient and secure
way to handle that and as a better

alternative there is a load balancer
service type
and the way it works with load balancer
service type is that the service becomes
accessible externally through a cloud
provider's load balancer functionality
so each cloud provider has its own
native load balancer implementation
and that is created and used whenever we
create a load balancer service type
google cloud platform aws azure linode
openstack and so on all of them offer
this functionality
so whenever we create a load balancer
node port and cluster ip services are
created automatically by kubernetes
to which the external load balancer of
the cloud platform
will route the traffic to and this is an
example of how did we define
load balancer service configuration so
instead of node port type we have a load
balancer
and the same way we have the port of the
service
which belongs to the cluster ip and we
have the node
port which is the port that opens on the
worker node
but it's not directly accessible
externally but only through the load
balancer itself
so the entry point becomes a load
balancer first and it can then
direct the traffic to node port
on the worker node and the cluster ip
the internal service
so that's how the flow would work with
the load balancer service
so in other words the load balancer
service type is an extension of the node
port type
which itself is an extension of the
cluster ip type
and again if i create a load balancer
service type
and list all the services you can see
the differences
in the display as well where for each
service type
you see the ip addresses you see the
type
and you see the ports that the service
has opened
and i should mention here that in a real

kubernetes setup example you would
probably not use node port for external
connection
you would maybe use it to test some
surveys very quickly
but not for production use cases so for
example if you have a
application that is accessible through
browser you will either configure
ingress for each such request so you
would have internal services the cluster
ip services that
ingress will route to or you would have
a load balancer that uses the cloud
platform's
native load balancer implementation

so that was an overview of kubernetes
service types
the differences between them and when to
use  
/////

